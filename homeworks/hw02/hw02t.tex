\documentclass[12pt]{article}

\include{preamble}

\newtoggle{professormode}
%\toggletrue{professormode} %STUDENTS: DELETE or COMMENT this line



\title{MATH 390.4 / 650.2 Spring 2020 Homework \#2}

\author{Tyron Samaroo} %STUDENTS: write your name here

\iftoggle{professormode}{
\date{Due 11:59PM Monday KY604, February 24, 2020 \\ \vspace{0.5cm} \small (this document last updated \currenttime~on \today)}
}

\renewcommand{\abstractname}{Instructions and Philosophy}

\begin{document}
\maketitle

\iftoggle{professormode}{
\begin{abstract}
The path to success in this class is to do many problems. Unlike other courses, exclusively doing reading(s) will not help. Coming to lecture is akin to watching workout videos; thinking about and solving problems on your own is the actual ``working out.''  Feel free to \qu{work out} with others; \textbf{I want you to work on this in groups.}

Reading is still \textit{required}. For this homework set, read the first chapter of \qu{Learning from Data} and Chapter 2 of Silver's book. Of course, you should be googling and reading about all the concepts introduced in class online. This is your responsibility to supplement in-class with your own readings.

The problems below are color coded: \ingreen{green} problems are considered \textit{easy} and marked \qu{[easy]}; \inorange{yellow} problems are considered \textit{intermediate} and marked \qu{[harder]}, \inred{red} problems are considered \textit{difficult} and marked \qu{[difficult]} and \inpurple{purple} problems are extra credit. The \textit{easy} problems are intended to be ``giveaways'' if you went to class. Do as much as you can of the others; I expect you to at least attempt the \textit{difficult} problems. 

This homework is worth 100 points but the point distribution will not be determined until after the due date. See syllabus for the policy on late homework.

Up to 7 points are given as a bonus if the homework is typed using \LaTeX. Links to instaling \LaTeX~and program for compiling \LaTeX~is found on the syllabus. You are encouraged to use \url{overleaf.com}. If you are handing in homework this way, read the comments in the code; there are two lines to comment out and you should replace my name with yours and write your section. The easiest way to use overleaf is to copy the raw text from hwxx.tex and preamble.tex into two new overleaf tex files with the same name. If you are asked to make drawings, you can take a picture of your handwritten drawing and insert them as figures or leave space using the \qu{$\backslash$vspace} command and draw them in after printing or attach them stapled.

The document is available with spaces for you to write your answers. If not using \LaTeX, print this document \textit{including this first page} and write in your answers. \inred{I do not accept homeworks which are \textit{not} on this printout.}

\end{abstract}

\thispagestyle{empty}
\vspace{1cm}
NAME: \line(1,0){380}
\clearpage
}

\problem{These are questions about Silver's book, chapter 2.}


\begin{enumerate}

\intermediatesubproblem{If one's goal is to fit a model for a phenomenon $y$, what is the difference between the approaches of the hedgehog and the fox? Answer using notation from class (i.e. $t ,f, g, h^*, \delta, \epsilon, e, t, z_1, \ldots, z_t, \mathbb{D}, \mathcal{H}, \mathcal{A}, \mathcal{X}, \mathcal{Y}, X, y, n, p, x_{\cdot 1}, \ldots, x_{\cdot p}, x_{1 \cdot}, \ldots, x_{n \cdot}$, etc.). Connecting this to the modeling framework should really make you think about what Tetlock's observation means for political and historical phenomena.}\spc{4}

Using the hedgehog approach when trying to fit a model for a phenomenon $y$ might be saying that their function \mathcal{g} is the unknown target function \mathcal{f} that predict the phenomenon.So hedgehog approach believe that  $g = f$. 

Using the fox approach when trying to fit a model for a phenomenon $y$ foxes know that its difficult to figure out the unknown target function $f$ so they know that they can choose from a hypothesis set  $\mathcal{H}$ apply a learning algorithm $\mathcal{H}$ to get a function $g$ that can best approximate the $f$ function.

\easysubproblem{Why did Harry Truman like hedgehogs? Are there a lot of people that think this way?}\spc{4}

Harry Truman like hedgehogs because because hedgehogs make quick answer, he did not like that he had foxes in his administration that couldn't give him a unqualified answer 


\hardsubproblem{Why is it that the more education one acquires, the less accurate one's predictions become?}\spc{4}


\easysubproblem{Why are probabilistic classifiers (i.e. algorithms that output functions that return probabilities) better than vanilla classifiers (i.e. algorithms that only return the class label)? We will move in this direction in class soon.}\spc{4}

\end{enumerate}


\problem{These are questions about the SVM.}

\begin{enumerate}

\easysubproblem{State the hypothesis set $\mathcal{H}$ inputted into the support vector machine algorithm. Is it different than the $\mathcal{H}$ used for $\mathcal{A}$ = perceptron learning algorithm?}\spc{1}

No the $\mathcal{H}$ is same that is used for $\mathcal{A}$ = perceptron learning algorithm

{$\mathcal{H} = \braces{\indic{w_o\xbar + b} > 0 : w \in R^p, b \in R }$}


\hardsubproblem{[MA] Prove the SVM converges. State all assumptions. Write it on a separate page.}\spc{0}

\hardsubproblem{Let $\mathcal{Y} = \braces{-1,1}$. Rederive the cost function whose minimization yields the SVM line in the linearly separable case. }\spc{20}
\begin{minipage}{0.45\textwidth}
Let $y_i = 1$
\newline
{w \cdot \xbar_i - (b + 1) \geq 0}
\newline
{w \cdot \xbar_i - b \geq 1}
\newline
$Multiply Both Sides by y_i$ 
\newline
{y_i w \cdot \xbar_i - b \geq 1(y_i)}
\newline
$Substitute y_i = 1$
\newline
{y_i w \cdot \xbar_i - b \geq 1}
\end{minipage}
\begin{minipage}{0.45\textwidth}
Let $y_i = -1$
\newline
{w \cdot \xbar_i - (b + 1) \leq 0}
\newline
{w \cdot \xbar_i - b \leq 1}
\newline
$Multiply Both Sides by y_i$ 
\newline
{y_i w \cdot \xbar_i - b \leq 1(y_i)}
\newline
$Substitute y_i = -1$
\newline
{y_i w \cdot \xbar_i - b \leq -1}
\newline
$Using inequlity rules$
\newline
{y_i w \cdot \xbar_i - b \geq 1}
\end{minipage}


So the cost function(Hinge Error) would be 
$H_i = max(0, \braces{(1 - y_i){w \cdot \xbar_i - b}})$
Which then will make the sum of hinge error(SHE) would be $$\sum_{n=1}^{n}{max(0, \braces{(1 - y_i){w \cdot \xbar_i - b}})}$$

\easysubproblem{Given your answer to (c) rederive the cost function using the \qu{soft margin} i.e. the hinge loss plus the term with the hyperparameter $\lambda$. This is marked easy since there is just one change from the expression given in class.}\spc{4}

\end{enumerate}



\problem{These are questions are about the $k$ nearest neighbors (KNN) algorithm.}

\begin{enumerate}

\easysubproblem{Describe how the algorithm works. Is $k$ a \qu{hyperparameter}?}\spc{5}

In KNN we have the function $g$ that finds the closes $\xbar_i$ and returns $y_i$. So we basically classify each points by its \qu{nearest neighbor} by the smallest distance.
We have ths Yes $k$ is a \qu{hyperparameter}. 

\hardsubproblem{Assuming $\mathcal{A} = $ KNN, describe the input $\mathcal{H}$ as best as you can.}\spc{8}

If our $\mathcal{A}$ is KNN out input $\mathcal{H}$ would be the same $\mathcal{H} = \braces{ \w. \xbar : \wbar \in  R^{p+1}}$ where $\mathcal{H}$ is the set of all linear models. 

\hardsubproblem{When predicting on $\mathbb{D}$ with $k=1$, why should there be zero error? Is this a good estimate of future error when new data comes in? (Error in the future is called \emph{generalization error} and we will be discussing this later in the semester).}\spc{5}



\end{enumerate}

\problem{These are questions about the linear model with $p=1$.}

\begin{enumerate}

\easysubproblem{What does $\mathbb{D}$ look like in the linear model with $p=1$? What is $\mathcal{X}$? What is $\mathcal{Y}$?}\spc{3}

The $\mathbb{D}$ looks like a line.$\mathcal{X}$ is input values x from $\mathbb{D}$ and our 
$\mathcal{Y}$ is output value y from $\mathcal{D}$

\easysubproblem{Consider the line fit using the ordinary least squares (OLS) algorithm. Prove that the point $<\xbar, \ybar>$ is on this line. Use the formulas we derived in class.}\spc{3}


{\sum{$(y_i - w_o -w_1x_i)^2$}} If we solve this the partial derivative of SSE with respect to partial $w_o$ is in the set of 0 and we get $w_0 = \ybar - w_1\xbar$ We solve SSE also with respect to partial $w_1$ in the set of 0 and we get $w_1 = \frac{\sum{x_iy_1 + n\xbar\ybar}}{\sum{x_i^2 + n\xbar^2}}$ This shows that the we will have the points $<\xbar, \ybar>$ on the line 

\intermediatesubproblem{Consider the line fit using OLS. Prove that the average prediction $\hat{y}_i := g(x_i)$ for $x_i \in \mathbb{D}$ is $\ybar$.}\spc{4}


\intermediatesubproblem{Consider the line fit using OLS. Prove that the average residual $e_i$ is 0 over $\mathbb{D}$.}\spc{3}

\intermediatesubproblem{Why is the RMSE usually a better indicator of predictive performance than $R^2$? Discuss in English.}\spc{4}


\intermediatesubproblem{$R^2$ is commonly interpreted as \qu{proportion of the variance explained by the model} and proportions are constrained to the interval $\zeroonecl$. While it is true that $R^2 \leq 1$ for all models, it is not true that $R^2 \geq 0$ for all models. Construct an explicit example $\mathbb{D}$ and create a linear model $g(x) = w_0 + w_1 x$ whose $R^2 < 0$.}\spc{10}

\intermediatesubproblem{[MA] Prove that the OLS line always has $R^2 \in \zeroonecl$ on a separate page.}

\hardsubproblem{You are given $\mathbb{D}$ with $n$ training points $<x_i, y_i>$ but now you are also given a set of weights $\bracks{w_1~w_2~ \ldots ~w_n}$ which indicate how costly the error is for each of the $i$ points. Rederive the least squares estimates $b_0$ and $b_1$ under this situation. Note that these estimates are called the \emph{weighted least squares regression} estimates. This variant $\mathcal{A}$ on OLS has a number of practical uses, especially in Economics. No need to simplify your answers like I did in class (i.e. you can leave in ugly sums).}\spc{12.5}


\intermediatesubproblem{[MA] Interpret the ugly sums in the $b_0$ and $b_1$ you derived above and compare them to the $b_0$ and $b_1$ estimates in OLS. Does it make sense each term should be altered in this matter given your goal in the weighted least squares?}\spc{5}


\hardsubproblem{[MA] In class we talked about $x_{raw} \in \braces{\text{red}, \text{green}}$ and the OLS model was the sample average of the inputted $x$ where $b_0 = \ybar_r$ and $b_1 = \ybar_g - \ybar_r$. Reparameterize $\mathcal{H} = \braces{w_1\indic{x_{raw} =~\text{red}}  + w_2 \indic{x_{raw} =~\text{green}}~:~ w_1, w_2 \in \reals}$ and prove that the OLS estimates are $b_1 = \ybar_r$ and $b_2 = \ybar_g$.}\spc{20}

\hardsubproblem{In class we talked about $x_{raw} \in \braces{\text{red}, \text{green}}$ and the OLS model was the sample average of the inputted $x$. Imagine if you have the additional constraint that $x_{raw}$ is ordinal e.g. $x_{raw} \in \braces{\text{low}, \text{high}}$ and you were forced to have a model where $g$(low) $\leq$ $g$(high). Invent an algorithm $\mathcal{A}$ that can solve this problem.}\spc{10}

\end{enumerate}

\problem{These are questions about association and correlation.}


\begin{enumerate}

\easysubproblem{Give an example of two variables that are both correlated and associated by drawing a plot.}\spc{7}

You can see a positive coloration here that as height increase so does age and visa versa. You can easily associate someone height based on their age visa versa.

\begin{tikzpicture}
\begin{axis}[
    enlargelimits=false,
    xlabel = $Age$,
    ylabel = $Height$,
    xmin=0, xmax=100,
    ymin=0, ymax=350,
]
\addplot+[
    only marks,
    scatter,
    mark=x,
    color=black,
    mark size=2.9pt]
table[meta=label] {
age height label
5  20   a
10 40 a
15 60 a
20 80 a
25 100 a
30 120 a
35 140 a
40 160 a
45 180 a
50 200 a
60 240 a
65 260 a
70 280 a
    };
\end{axis}
\end{tikzpicture}


\easysubproblem{Give an example of two variables that are not correlated but are associated by drawing a plot.}\spc{10}
 
You cannot tell that the meals you eat will determine the amount of time a person will study but you can associate those who eat one meal to around 20-40 minutes of study time 

\begin{tikzpicture}
\begin{axis}[
    enlargelimits=false,
    xlabel = {Meals A Day},
    ylabel = {Study Time (Min)},
    xmin=0, xmax=5,
    ymin=0, ymax=150,
]
\addplot+[
    only marks,
    scatter,
    mark=x,
    color=black,
    mark size=2.9pt]
table[meta=label] {
age height label
1  40   a
1 30 a
1 20 a
1 35 a
1 20 a
1 36 a
1 17 a
1 30 a
1 28 a
1 15 a
1 15 a
1 35 a
1 30 a
4 20 a
4 20 a
4 25 a
4 45 a
4 50 a
4 37 a
3 100 a
3 96 a
3 104 a
3 130 a
3 80 a
3 78 a
4 92 a


    };
\end{axis}
\end{tikzpicture}


\easysubproblem{Give an example of two variables that are not correlated nor associated by drawing a plot.}\spc{10}


\begin{tikzpicture}
\begin{axis}[
    enlargelimits=false,
    xlabel = {Number of Cars Counted},
    ylabel = {Cash I Have},
    xmin=0, xmax=200,
    ymin=0, ymax=200,
]
\addplot+[
    only marks,
    scatter,
    mark=x,
    color=black,
    mark size=2.9pt]
table[meta=label] {
age height label
5  5   a
60,60 a
30 10 a
10 30 a
25 100 a
30 17 a
80 140 a
40 160 a
8 150 a
50 200 a
60 240 a
280 100 a
150 150 a
150 100 a
180 60 a
100 100 a
50 50 a
150 30 a 
150 20 a
100 40 a
100 50 a
    };
\end{axis}
\end{tikzpicture}


\easysubproblem{Can two variables be correlated but not associated? Explain.}\spc{5}

Yes two variables can be correlated but not associated. For example you can say there's correlation between people who died and drank water.But you can associated dying with drinking water since you are ignoring that there's tons of other reason why people die.  

\hardsubproblem{[MA] Prove association $\centernot\implies$ correlation. This requires some probability theory.}\spc{5}

\end{enumerate}


\end{document}

\pagebreak
We want to prove $\XtX$ is invertible only when $\rank{\X} = p + 1$ i.e. the design matrix is \qu{full rank}. This is equivalent to proving that 

\beqn
\rank{\X} = p + 1 \implies \rank{\XtX} = p + 1
\eeqn

Logically equivalent is the contrapositive:

\beqn
\rank{\XtX} \neq p + 1 \implies \rank{\X} \neq p + 1
\eeqn

Not equal in this case is equivalent to less than because a matrix cannot have a rank that exceeds its number of columns since $\rank{\X} := \dim{\colsp{\X}}$ so the above is equivalent to:

\beqn
\rank{\XtX} < p + 1 \implies \rank{\X} < p + 1
\eeqn

Beginning with the premise on the left hand side, a rank-deficient matrix has at least one non-trivial (i.e. non-zero) vector $\v \in \reals^{p+1}$ that maps to the zero vector, i.e. there is at least one direction in the nullspace:

\beqn
\XtX\v = \bv{0}_{p+1}
\eeqn

Saqib noticed we can multiply both sides on the right by $\v^\top$ to arrive at:


\beqn
\v^\top\XtX\v = \v^\top\bv{0}_{p+1} = 0
\eeqn

We can write this alternatively as

\beqn
 (\X\v)^\top \X\v = 0 \implies \sum_{i=1}^n (\X\v)_i^2 = 0
\eeqn

If all elements of $\X\v$ squared and summed yield zero, every single element must be zero and thus,

\beqn
\X\v = \bv{0}_n
\eeqn

indicating that the vector $\v$ (which was assumed to be nontrivial above) is in the nullspace of $\X$ indicating that $\X$ is rank deficient and thus $\rank{\X} < p + 1.~ \blacksquare$ \pagestyle{empty}
